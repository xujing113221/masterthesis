\chapter{Conclusions and Future Work}
\label{chap:conclusion}
Our final model Retina Net are all based on transformer structure,including two most important processes in visual relation detection: object detection and relation prediction. we have abandoned the traditional Faster R-CNN and its cumbersome NMS, making the model simpler with fewer parameters. We have also made appropriate modification to the transformer structure and defined a more practical object query with physical meaning. Our new object query is more suitable for completing PredCLS and SGCLS tasks. Our attention loss has also completed the task excellently, which makes our object feature more recognisable and can be visualised through the corresponding attention map. Last but not least, through attention mechanism our relation decoder  realize the context between the relation to objects,which significantly improved the final results.

Before our graduation, we attempt to  find the relationship between different objects through pixel-wise attention. which is a newly brought-up idea. But we have proved through experiment that it is inoperable, because every picture has a serious overlap, which makes our training meaningless. Although we have failed, but it makes us to understand our task better, and we also learned a lot from it. After this approach we immediately adjusted our strategy and made a comprehensive revision of our work. Eventually, we achieved a breakthrough in box-wise and completely solved the problem of visual relation detection.

Although we have completely solved our problems, but there are still some shortcomings, for example, our recall score is not as good as state-of-the-art performance in all tasks, which means that our models could use some work for future improvement .  The attention mechanism in transformer has some advantages of relation pair filtering, we ought to find the most important note in neural network through attention between each object, then find the most related relation pair. In this way, the scores of recall@20 can be improved.

Through experiment we also have found that our dataset is not as good as expected. There are many labeling errors and receptions, and some of the predicates are uneven distributed, and some serious long tail problems as well. Therefore, the data augmentation is also absolutely necessary.