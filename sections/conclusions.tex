\chapter{Conclusions and Future Work}
\label{chap:conclusion}
Our final model Retina Net is based on transformer structure,including two most important processes, object detection and relation prediction,  in visual relation detection. The traditional Faster R-CNN and its complex NMS are abandoned, making the model simpler with fewer parameters. Appropriate modification is made to the transformer structure and a more practical object query with physical meaning is defined. Our new object query is more suitable for completing PredCLS and SGCLS, and the attention loss has also completed the task excellently, which makes our object feature more recognizable and can be visualised through the corresponding attention map. Last but not least, through attention mechanism the relation decoder  realize the context between the relation to objects,which significantly improved the final results.

In the beginning of the master thesis, we attempt to  find the relationship between different objects through pixel-wise attention, which is a newly brought-up idea. But it is proved through experiment that it's inoperable, because every picture has a serious overlap, which makes the training meaningless. Although this method is not feasible, it makes us understand the task better, and we also learned a lot from it. After the failure of this approach we immediately adjusted our strategy and made a comprehensive revision of our work. Eventually, we achieved a breakthrough in box-wise and completely solved the problem of visual relation detection.

Although we have completely solved our problems, but there are still some shortcomings. Our recall score is not as good as state-of-the-art performance in all tasks, which means that our models could still be improved in the future. For instance, the attention mechanism in transformer has some advantages of relation pair filtering, we ought to find the most important note in neural network through attention between each object, then find the most related relation pair. In this way, the scores of recall@20 can be improved.

Through experiment we also have found that our dataset is not as good as expected. There are many labeling errors and receptions, and some of the predicates are uneven distributed, and some serious long tail problems as well. Therefore, the data augmentation is also absolutely necessary.