\chapter*{Abstract}
\label{chap:Abstract}
\addcontentsline{toc}{chapter}{Abstract}
\setcounter{page}{1}
\pagenumbering{roman}
%******************************************************************************
Visual relationship detection(VRD) aims at inferring the interactions between objects in an image. Compared to the task of object detection, it requires greater effort due to the large number of possible pairs and various synonyms. Visual relationship detection plays an important role in scene understanding and it can also connect the computer vision and natural language. 

In this thesis we mainly discuss the solution of visual relation detection. Inspired by the attention mechanism in computer vision, we propose Reltina-Net, a Transformer-based model which consists of 4 modules: (1)A convolutional neural network (CNN) backbone and an \textit{Encoder} module generate visual features of the images; (2)A \textit{Object Decoder} module is to obtain object features and  the context between each entity; (3)A \textit{Relation Decoder} module is to obtain the context between relations and entities;(4) A \textit{Predicate Classifer} module is applied to predict predicates. Furthermore, we introduce an encoding method for the object queries with highly interpretability in order to evaluate the performance of different modules. Our specific object queries is capable of extracting better visual features from an image with the attention loss. 

It is a new approach to solve the VRD problem by using the Transformer structure.  Through experiments we verify the consistency of our object queries and objects, and it performs well in predicate classification and scene graph detection. Our attention loss function can make the object feature more focused on itself, which improves its quality. And our object context and relation context can improve the recall rate of the visual relationship detection.


%In this thesis we mainly discuss  the solutions for visual relation detection. To find the solutions, we not only need to  develop models that detect the objects in an image, but also need to predict the interactions between the detected objects. Therefore, a comprehensive scene understanding of an image would be considered in aspect of connecting computer vision and natural language  helpful. Even though the work in area of deep learning has been quite successful, it remains to be a complex and challenging task to extract related scene information. 
%
% Inspired by the contemporary achievement of transformer in computer vision, we propose Retina Net, which was based transformer structure. In order to meet different requirements in visual relation detection, we designed specific object queries with physical meanings. The specific object queries is capable of extracting better object features in an image with the help of our attention loss.  By considering the global context, we also model relation to object interactions through the relation decoder. We attempt a lot to find the mechanism of attention in the models and to understand which details should be paid more attention to when it comes to relation predicts.
% 
% To our best effort, our study for the first time try to complete all the models of relation detection . It is a new approach to solve the problems by using transformer structure. The results of our experiments indicate that our models solve the tasks in visual relation detection properly.

%------------------------------------------------------------------------
%-------------------------------------------------------------------------

%******************************************************************************
\chapter*{Kurzfassung}

%------------------------------------------------------------------------
%-------------------------------------------------------------------------

%******************************************************************************
\label{chap:Kurzfassung}
\addcontentsline{toc}{chapter}{Kurzfassung}

Visual Relationship Detection (VRD) zielt darauf ab, die Interaktionen zwischen Objekten in einem Bild abzuleiten. Im Vergleich zur Aufgabe der Objekterkennung erfordert VRD aufgrund der Vielzahl m"oglichen Paare und verschiedener Synonyme einen h"oheren Aufwand. Die Erkennung von visuellen Beziehungen spielt eine wichtige Rolle in der Szene, es kann auch die Computer Vision und die nat"urliche Sprache verbinden.

In dieser Arbeit diskutieren wir haupts"achlich die L"osung der Gesichtserkennung. Inspiriert vom Aufmerksamkeitsmechanismus in der Computer Vision schlagen wir Reltina-Net vor, ein Transformer-basiertes Modell, das aus 4 Modulen besteht: (1) Ein Convolutional Neural Network (CNN) Backbone und ein Encoder Modul erzeugen visuelle Merkmale der Bilder; (2) Ein Object Decoder Modul soll Objektmerkmale und den Kontext zwischen jeder Entit"at erhalten; (3) Ein Relation Decoder Modul soll den Kontext zwischen Relationen und Entit"aten erhalten; (4) Zur Vorhersage von Pr"adikaten wird ein Pr"adikaten Classifier angewendet.

Es ist ein neuer Ansatz zur L"osung des VRD-Problems unter Verwendung der Transformer-Struktur. Durch Experimente "uberpr"ufen wir die Konsistenz unserer Objektabfragen und Objekte, und es funktioniert gut bei der Pr"adikatenklassifizierung und der Erkennung von Szenengraphen. Unsere Aufmerksamkeitsverlustfunktion kann das Objektmerkmal st"arker auf sich selbst konzentrieren, was seine Qualit"at verbessert.  Und unser Objektkontext und Beziehungskontext k"onen die Erinnerungsrate der visuellen Beziehungserkennung verbessern.