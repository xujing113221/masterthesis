\chapter{Related Work}
\label{chap:relatedwork}
With the development of deep neural network~\cite{krizhevsky2012imagenet,he2016deep}, a lot of models that solve computer vision problems spring up and are already applied in our daily life. Some of them have made outstanding contributions to object detection which is fundamental for other scene understanding tasks. As a result, visual relationship detection, which is helpful for high level semantic tasks such as image retrieval~\cite{johnson2015image} and visual question answering~\cite{ren2015exploring}, achieves more and more attention. 

In this chapter, we discuss about the development of object detection at first. Then we introduce and compare some representative approaches of visual relationship detection in brief. Furthermore, we give a short introduction for neural ordinary differential equations which are used in our model.

\section{Object Detection}
%Object detection is more challenging than image classification. In object detection tasks, it is necessary to localize every objects in an image before classifying them. But even so, this hard problem is being solved gradually with the power of convolutional neural networks (CNNs). As one of the first models using CNNs for object detection, Sermanet et al. [14] implemented a multiscale and sliding window approach within CNNs. Girshick et al. [15] also combined region proposals generated by selective search with CNNs (R-CNN) and achieved a mAP of 53.3\% on VOC 2012. However, R-CNN has to give all region proposals through CNNs to extract features. In order to reduce the amount of computation, SPPnet [16] was integrated to share computation [17]. Based on Fast R-CNN, Ren et al. [4] introduced region proposal network and achieved the end-to-end Faster R-CNN which has a good performance on mAP and running speed. Because of its outstanding advantages, almost all of the models in visual relationship detection use Faster R-CNN as the object detector [18, 19, 1, 20].

Object detection is more challenging than image classification. In object detection tasks, it is necessary to localize each object in the image before classifying the image. But even so, with the help of convolutional neural networks (CNN), this problem is gradually being solved. Base on Fast R-CNN~\cite{girshick2015fast}, Ren et al. Introduced the regional proposal network and realized the end-to-end Faster R-CNN~\cite{ren2016faster}, which has good performance in mAP and operating speed. Due to its outstanding advantages, almost all visual relationship detection models use Faster R-CNN as the target detector ~\cite{yang2018graph, zellers2018neural, zhang2019graphical}. Carion et al. proposed a very novel model Detr~\cite{carion2020end} based on the structure of the transformer encoder decoder, an end-to-end object detection model, which makes the transformer shine in the field of computer vision. It has a simpler model structure and fewer model parameters, but it has the same effect as Faster R-CNN. This provides more options for the area of computer vision.

\section{Visual Relationship Detection}
Among many visual tasks, visual relationship detection has been proven to be a useful technical tool. it is based on object detection and predicts the interaction between objects. along with scene graph construction visual relationship detection is considered to be a well-known research direction in visual scene understanding.

The concept of scene graph was given definition by Johnson et al. in their work the contents of a scene~\cite{johnson2015image} was explained. later on these contents were widely used in visual relationship detection tasks. Lu et al. developed a dataset for visual relationship detection and proposed a model leveraging visual appearance and language priors~\cite{girshick2015fast}. With visual relationship detection developing, Visual Genome~\cite{krishna2017visual} is challenged by most models. which is considered more complicated. Zhang et al.~\cite{zhang2017visual} brought up a new model, which use end-to-end technique. These end-to-end model places objects in a low-dimensional relation space where vector translation represent a relation. Li et al. developed scene graph with the informations that are learned from image caption~\cite{li2017scene}.

Take look into the semantic connection between objects and their relationship, Xu et al.~\cite{xu2017scene} proposed a RNNs-based model,  which leans via message passing. Liao et al.~\cite{liao2019exploring} refined the features of objects and relations by using source-target cognitive transformations. for the realisation of filtering the redundant pairs, Yang et al.~\cite{yang2018graph} brought up a relation proposal network(RePN) that absorbed inspiration from region proposal network(RPN)of Faster R-CNN~\cite{ren2016faster}. Zeller et al.~\cite{zellers2018neural} developed a LSTMs based model, which is capable of learning global informations along with its analysis on Visual Genome. the work has shown that the relationship classes are massively predicted by object classes, but not vice-versa[motifs]. Liang et al. designed a new ranking objective function. through this newly designed function the labeled relationship was enforced to obtain a higher relevance scores~\cite{liang2018visual}.

\section{Context}
Many methods have been proposed for modeling semantic context in object recognition~\cite{divvala2009empirical}. The Motifs Net~\cite{zellers2018neural} is most closely related to work that models object co-occurrence using graphical models to combine many sources of contextual information ~\cite{rabinovich2007objects,belongie2007context}. It is unique in that it stages incorporation of context allowing for meaningful global context from large conditioning sets. It represents the global context via recurrent sequential architectures such as Long Short-term Memory Networks (LSTMs)~\cite{hochreiter1997long}.

Actions and relations have been a particularly fruitful source of context~\cite{marszalek2009actions,gupta2015visual}, especially when combined with pose to create human-object interactions~\cite{yao2010modeling}. Recent work has shown that object layouts can provide sufficient context for captioning COCO images ~\cite{lin2014microsoft}.