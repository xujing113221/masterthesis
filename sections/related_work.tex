\chapter{Related Work}
\label{chap:relatedwork}
With the development of deep neural network [2, 11], a lot of models that solve computer vision problems spring up and are already applied in our daily life. Some of them have made outstanding contributions to object detection which is fundamental for other scene understan- ding tasks. As a result, visual relationship detection, which is helpful for high level semantic tasks such as image retrieval [12] and visual question answering [13], achieves more and more attention.

In this chapter, we discuss about the development of object detection at first. Then we intro- duce and compare some representative approaches of visual relationship detection in brief. Furthermore, we give a short introduction for neural ordinary differential equations which are used in our model.

\section{Object Detection}
Object detection is more challenging than image classification. In object detection tasks, it is necessary to localize every objects in an image before classifying them. But even so, this hard problem is being solved gradually with the power of convolutional neural networks (CNNs). As one of the first models using CNNs for object detection, Sermanet et al. [14] implemented a multiscale and sliding window approach within CNNs. Girshick et al. [15] also combined region proposals generated by selective search with CNNs (R-CNN) and achieved a mAP of 53.3\% on VOC 2012. However, R-CNN has to give all region proposals through CNNs to extract features. In order to reduce the amount of computation, SPPnet [16] was integrated to share computation [17]. Based on Fast R-CNN, Ren et al. [4] introduced region proposal network and achieved the end-to-end Faster R-CNN which has a good performance on mAP and running speed. Because of its outstanding advantages, almost all of the models in visual relationship detection use Faster R-CNN as the object detector [18, 19, 1, 20].

\section{Visual Relationship Detection}

We follow three standard evaluation modes: (1) \textbf{predicate classification} (PREDCLS): given a ground truth set of boxes and labels, predict edge labels, (2) \textbf{scene graph classification} (SGCLS): given ground truth boxes, predict box labels and edge label and (3) \textbf{scene graph detection} (SGDET): predict boxes, box labels, and edge labels.


\section{Context}
Many methods have been proposed for modeling semantic context in object recognition [7]. Our approach is most closely related to work that models object co-occurrence using graphical models to combine many sources of contextual information [33, 11, 26, 10]. While our approach is a type of graphical model, it is unique in that it stages incorporation of context allowing for meaningful global context from large conditioning sets.\\
Actions and relations have been a particularly fruitful source of context [30, 50], especially when combined with pose to create human-object interactions [48, 3]. Recent work has shown that object layouts can provide sufficient context for captioning COCO images [52, 28]; our work suggests the same for parsing Visual Genome scene graphs. Much of the context we derive could be interpreted as commonsense priors, which have commonly been extracted us- ing auxiliary means [59, 39, 5, 49, 55]. Yet for scene graphs, we are able to directly extract such knowledge.